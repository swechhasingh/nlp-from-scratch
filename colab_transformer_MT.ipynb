{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swechhasingh/nlp-from-scratch/blob/main/colab_transformer_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUQmX6mSW3Jo"
      },
      "source": [
        "### English-French Neural Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL3dE0XzW3Jp"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fuq5OkRCW3Jq",
        "outputId": "06dd3bc7-7fdc-44b7-844c-cf36d38f89e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPS not available because the current PyTorch install was not built with MPS enabled.\n"
          ]
        }
      ],
      "source": [
        "# Check that MPS is available\n",
        "if not torch.backends.mps.is_available():\n",
        "    if not torch.backends.mps.is_built():\n",
        "        print(\"MPS not available because the current PyTorch install was not \"\n",
        "              \"built with MPS enabled.\")\n",
        "    else:\n",
        "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
        "              \"and/or you do not have an MPS-enabled device on this machine.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYbxTQsXW3Jq",
        "outputId": "bcfd544b-1711-4e69-84f6-10dcb1600b36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"mps\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYP1gEp3W3Jr"
      },
      "outputs": [],
      "source": [
        "src_language = 'eng'\n",
        "target_language = 'fra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDlbvXFbW3Jr",
        "outputId": "12bf5e4b-03b4-4a2c-a979-818741e9602f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Go.\\tVa !', 'Run!\\tCours\\u202f!', 'Run!\\tCourez\\u202f!', 'Wow!\\tÇa alors\\u202f!', 'Fire!\\tAu feu !']\n"
          ]
        }
      ],
      "source": [
        "# Read the file and split into lines\n",
        "with open('data/%s-%s.txt' % (src_language, target_language), encoding='utf-8') as file:\n",
        "    text_data = file.read().splitlines()\n",
        "print(text_data[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHrWihehW3Jr"
      },
      "source": [
        "The text is in Unicode. So, we will take the following preprocessing steps:\n",
        "1. Turn Unicode characters to ASCII\n",
        "2. lowercase\n",
        "3. Trim most punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsi0BliwW3Jr"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(sent):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', sent)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def preprocess_string(sent):\n",
        "    \"lowercase, unicode_to_ascii, trim, and remove non-letter characters\"\n",
        "    sent = sent.lower().strip()\n",
        "    sent = unicode_to_ascii(sent)\n",
        "    # The backreference \\1 (backslash one) references the first capturing group. \n",
        "    # space followed by \\1 matches the exact same text that was matched by the first capturing group [.!?].\n",
        "    sent = re.sub(r\"([.!?])\", r\" \\1\", sent)\n",
        "    # replace character which are not from this set (a-zA-Z.!?) by single space character\n",
        "    sent = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sent)\n",
        "    return sent.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flDteWRLW3Js",
        "outputId": "bdde704d-fc62-4922-b72b-ff9afd3ee003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wow!\tÇa alors !\n",
            "['Ça alors\\u202f!', 'Wow!']\n"
          ]
        }
      ],
      "source": [
        "preprocess_string(text_data[3])\n",
        "print(text_data[3])\n",
        "print(text_data[3].split('\\t')[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcFEWkDSW3Js"
      },
      "outputs": [],
      "source": [
        "def load_data(file_name, reverse=False):\n",
        "    print(\"Reading text file...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    with open('data/%s' % (file_name), encoding='utf-8') as file:\n",
        "        lines = file.read().splitlines()\n",
        "\n",
        "    # Split every line into pairs [src_lang, target_lang] and preprocess\n",
        "    pairs = [[preprocess_string(s) for s in line.split('\\t')] for line in lines]\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [p[::-1] for p in pairs]\n",
        "        \n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zKwXEPCW3Js",
        "outputId": "64ff756f-af9c-4a71-9a0e-30b2745df2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading text file...\n"
          ]
        }
      ],
      "source": [
        "pairs = load_data('eng-fra.txt', reverse=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLlMKRkWW3Js"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK = 2\n",
        "PAD = 3\n",
        "BLOCK_SIZE = 12\n",
        "\n",
        "class Language:\n",
        "    def __init__(self, lang_name, src=True):\n",
        "        self.lang_name = lang_name\n",
        "        self.word_to_index = {\"SOS\":0, \"EOS\":1, \"UNK\": 2, \"PAD\":3}\n",
        "        self.index_to_word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\", 3:\"PAD\"}\n",
        "        self.word_to_count = {}\n",
        "        self.vocab_size = 4\n",
        "        self.src = src\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word_to_index:\n",
        "            self.word_to_index[word] = self.vocab_size\n",
        "            self.index_to_word[self.vocab_size] = word\n",
        "            self.vocab_size += 1\n",
        "        self.word_to_count[word] = self.word_to_count.get(word, 0) + 1\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def sentence_to_indexes(self, sentence):\n",
        "        idxs = [self.word_to_index[word] if word in self.word_to_index else self.word_to_index[\"UNK\"] for word in sentence.split(' ')]\n",
        "        return idxs\n",
        "        \n",
        "    def indexes_to_sentence(self, indexes):\n",
        "        return ' '.join([self.index_to_word[index] for index in indexes])\n",
        "\n",
        "    def sentence_to_tensor(self, sentence):\n",
        "        indexes = self.sentence_to_indexes(sentence)\n",
        "        indexes = [SOS_token] + indexes + [EOS_token]\n",
        "\n",
        "        max_len = BLOCK_SIZE if self.src else BLOCK_SIZE + 1\n",
        "        \n",
        "        if len(indexes) < max_len:\n",
        "            indexes += [PAD]*(max_len-len(indexes))\n",
        "        else:\n",
        "            indexes = indexes[:max_len]\n",
        "            \n",
        "        indexes = torch.tensor(indexes, dtype=torch.long)\n",
        "        return indexes\n",
        "\n",
        "    def tensor_to_sentence(self, idx_tensor):\n",
        "        if len(idx_tensor.shape) > 1:\n",
        "            idxs = idx_tensor.tolist()[0]\n",
        "        else:\n",
        "            idxs = idx_tensor.tolist()\n",
        "        sentence = self.indexes_to_sentence(idxs)\n",
        "        return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO2pdabRW3Jt"
      },
      "outputs": [],
      "source": [
        "# create language instances\n",
        "src_lang = Language(src_language)\n",
        "target_lang = Language(target_language)\n",
        "\n",
        "for src, target in pairs:\n",
        "    src_lang.add_sentence(src)\n",
        "    target_lang.add_sentence(target) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nZesrsgW3Jt"
      },
      "source": [
        "Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYDeVCulW3Jt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXLu25TjW3Jt"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p, reverse=False):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1 if reverse else 0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs, reverse):\n",
        "    return [pair for pair in pairs if filterPair(pair, reverse)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0cyjbJEW3Jt"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "Read text file and split into lines, split lines into pairs\n",
        "\n",
        "Normalize text, filter by length and content\n",
        "\n",
        "Make word lists from sentences in pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqu3b83qW3Jt"
      },
      "outputs": [],
      "source": [
        "def create_dataset(src_lang, target_lang, reverse=False):\n",
        "    pairs = load_data('eng-fra.txt', reverse)\n",
        "\n",
        "    # create language instances\n",
        "    input_lang = Language(src_lang)\n",
        "    output_lang = Language(target_lang, src=False)\n",
        "\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    pairs = filterPairs(pairs, reverse)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    # train/val/test split\n",
        "    n_total = len(pairs)\n",
        "    n_train = int(0.8*n_total)\n",
        "    n_val = int(0.1*n_total)\n",
        "    n_test = n_total - n_train - n_val\n",
        "    print(f\"{n_train=}, {n_val=}, {n_test=}\")\n",
        "    pair_split = {}\n",
        "    pair_split['train'] = pairs[:n_train]\n",
        "    pair_split['val'] = pairs[n_train:n_train + n_val]\n",
        "    pair_split['test'] = pairs[n_train + n_val:]\n",
        "    \n",
        "    print(\"Counting words...\")\n",
        "    print(\"Creating source and target language vocab using pair_split['train']...\")\n",
        "    for src, target in pair_split['train']:\n",
        "        input_lang.add_sentence(src)\n",
        "        output_lang.add_sentence(target) \n",
        "\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.lang_name, input_lang.vocab_size)\n",
        "    print(output_lang.lang_name, output_lang.vocab_size)\n",
        "    return input_lang, output_lang, pair_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk-EpzmlW3Ju",
        "outputId": "57128002-1793-4904-a2fc-cc213c1c63de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading text file...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "n_train=8479, n_val=1059, n_test=1061\n",
            "Counting words...\n",
            "Creating source and target language vocab using pair_split['train']...\n",
            "Counted words:\n",
            "eng 2184\n",
            "fra 3526\n",
            "['you re silly .', 'vous etes idiots .']\n"
          ]
        }
      ],
      "source": [
        "src_lang, target_lang, pair_split = create_dataset('eng', 'fra', reverse=False)\n",
        "print(random.choice(pair_split[\"train\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dopRngz7W3Ju"
      },
      "source": [
        "#### Tokenization: Words to indexes\n",
        "In seq2seq task takes an input sequence (source seq) and outputs another sequence (target seq). In our case, we have an input sentence in English language and a corresponding translated sentence in French language. These sentence need to converted into numbers (integers) to be able to into to a neural network. For this, we will use **word-level tokenization**, i.e *word to integer* index mapping.\n",
        "\n",
        "We need some special tokens to indicate start (SOS) and end (EOS) of a sentence. For the input sequence (source seq), the model needs to know when the input has ended and for the target sequence, the model needs to know when to start and when to end.\n",
        "\n",
        "So, we will append the EOS token to the end of input sentence and wrap the target sentence by SOS (in the beginning) and the EOS (in the end) tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_1q9Ph9W3Ju"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sent_pair_to_tensor_pair(pair, src_lang, target_lang):\n",
        "    input_tensor = src_lang.sentence_to_tensor(pair[0])\n",
        "    target_tensor = target_lang.sentence_to_tensor(pair[1])\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "def tensor_pair_to_sent_pair(pair, src_lang, target_lang):\n",
        "    input_tensor = src_lang.tensor_to_sentence(pair[0])\n",
        "    target_tensor = target_lang.tensor_to_sentence(pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqfYklMAW3Ju",
        "outputId": "ab670087-b37b-4a3b-9038-1430e30e5059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['they re all fake .', 'ils sont tous faux .']\n",
            "(tensor([  0, 223,  80, 157, 809,   6,   1,   3,   3,   3,   3,   3]), tensor([   0,  350,  351,  547, 1465,    7,    1,    3,    3,    3,    3,    3,\n",
            "           3]))\n",
            "(tensor([  0, 223,  80, 157, 809,   6,   1,   3,   3,   3,   3,   3]), tensor([   0,  350,  351,  547, 1465,    7,    1,    3,    3,    3,    3,    3,\n",
            "           3]))\n",
            "('SOS they re all fake . EOS PAD PAD PAD PAD PAD', 'SOS ils sont tous faux . EOS PAD PAD PAD PAD PAD PAD')\n"
          ]
        }
      ],
      "source": [
        "pair = random.choices(pair_split[\"train\"], k=1)[0]\n",
        "print(pair)\n",
        "print(sent_pair_to_tensor_pair(pair, src_lang, target_lang))\n",
        "print(sent_pair_to_tensor_pair(pair, src_lang, target_lang))\n",
        "\n",
        "p1, p2 = sent_pair_to_tensor_pair(pair, src_lang, target_lang)\n",
        "print(tensor_pair_to_sent_pair((p1,p2), src_lang, target_lang))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkucay4VW3Ju"
      },
      "outputs": [],
      "source": [
        "class Batch:\n",
        "    def __init__(self, src, target=None, pad_idx=2) -> None:\n",
        "        # src and target sequences have same length\n",
        "        self.src = src #shape:(B,T)\n",
        "        # src_mask:(B,1,1,T)\n",
        "        self.src_mask = (src != pad_idx).unsqueeze(-2).unsqueeze(-2)\n",
        "        if target is not None:\n",
        "            # decoder output shifted by one\n",
        "            self.tgt = target[:,:-1] #shape:(B,T)\n",
        "            self.tgt_y = target[:,1:]\n",
        "            # padding mask: (B,T)\n",
        "            self.tgt_pad_mask = (self.tgt_y != pad_idx)\n",
        "            self.tgt_mask = self.causal_mask(self.tgt, pad_idx)\n",
        "            self.n_tokens = self.tgt_pad_mask.sum().item()\n",
        "\n",
        "    @staticmethod   \n",
        "    def causal_mask(target, pad_idx):\n",
        "        # max context length = block_size\n",
        "        T = target.shape[1]\n",
        "        # causal attention mask: (1,T,T)\n",
        "        causal_attn_mask = torch.tril(torch.ones(1, T, T, dtype=torch.bool, device=target.device))\n",
        "        # padding mask: (B,1,T)\n",
        "        tgt_pad_mask = (target != pad_idx).unsqueeze(-2)\n",
        "        target_mask = tgt_pad_mask & causal_attn_mask\n",
        "        return target_mask.unsqueeze(1) # (B,1,T,T)\n",
        "\n",
        "        \n",
        "def build_batch(split, batch_size=4):\n",
        "    if split == \"train\":\n",
        "        pairs = pair_split[\"train\"]\n",
        "    elif split == \"val\":\n",
        "        pairs = pair_split[\"val\"]\n",
        "    else:\n",
        "        pairs = pair_split[\"test\"]\n",
        "    # randomly (uniformly) sample a start index for a sentence of length block_size\n",
        "    # number of sequences in a batch is batch_size\n",
        "    batch_pairs = random.choices(pairs, k=batch_size)\n",
        "    # input \n",
        "    src_batch = []\n",
        "    target_batch = []\n",
        "    \n",
        "    for pair in batch_pairs:\n",
        "        src_tensor, target_tensor = sent_pair_to_tensor_pair(pair, src_lang, target_lang)\n",
        "        src_batch.append(src_tensor)\n",
        "        target_batch.append(target_tensor)\n",
        "    \n",
        "    src = torch.stack(src_batch).to(device)\n",
        "    target = torch.stack(target_batch).to(device)\n",
        "\n",
        "    batch = Batch(src, target, PAD)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3ptKYyQW3Ju"
      },
      "outputs": [],
      "source": [
        "b = build_batch(\"train\", batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfMMifvOW3Ju",
        "outputId": "888909a3-e713-430a-9b2f-3250c4a994b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SOS you are a bit fat . EOS PAD PAD PAD PAD\n",
            "SOS tu es un peu gras . EOS PAD PAD PAD PAD\n",
            "tu es un peu gras . EOS PAD PAD PAD PAD PAD\n"
          ]
        }
      ],
      "source": [
        "print(src_lang.tensor_to_sentence(b.src))\n",
        "print(target_lang.tensor_to_sentence(b.tgt))\n",
        "print(target_lang.tensor_to_sentence(b.tgt_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTUpb-xMW3Jv"
      },
      "source": [
        "#### Seq2Seq English-French Machine Translation Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDoThXdUW3Jv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-neO0ibaW3Jv"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "from transformers.transformer import EncDecTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlLHA3R5W3Jv"
      },
      "outputs": [],
      "source": [
        "# model hyperparameters\n",
        "model_hyparam = {\n",
        "    \"src_vocab_size\": src_lang.vocab_size,\n",
        "    \"tgt_vocab_size\": target_lang.vocab_size,\n",
        "    \"block_size\": BLOCK_SIZE,\n",
        "    \"model_dim\": 128,\n",
        "    \"n_layer\": 4,\n",
        "    \"n_head\": 4,\n",
        "    \"cross_attention\":True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asx7K1LUW3Jv",
        "outputId": "54b490c8-1249-4777-cc8b-315da4586112"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Batch at 0x7ff22f2d4760>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "batch = build_batch(\"train\", batch_size=4)\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPl4Tp59W3Jv"
      },
      "outputs": [],
      "source": [
        "# training hyperparameters\n",
        "batch_size = 16\n",
        "n_iters = 2000\n",
        "eval_interval = 10\n",
        "base_lr = 1.0\n",
        "eval_iters = 100\n",
        "warmup_steps = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn-lvaaXW3Jv"
      },
      "source": [
        "Vary the learning rate over the course of training, according to the formula:<br>\n",
        "<br>\n",
        "$lr = {d_{model}}^{-0.5}*\\min({step\\_num}^{-0.5}, step\\_num * {warmup\\_steps}^{-1.5})$\n",
        "<br>\n",
        "This corresponds to increasing the learning rate linearly for the first $warmup\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34gGxu3DW3Jv"
      },
      "outputs": [],
      "source": [
        "def learning_rate(step, model_size, factor, warmup):\n",
        "    # default the step to 1 for LambdaLR function to avoid zero raising to negative power\n",
        "    if step == 0:\n",
        "        step = 1\n",
        "    return factor * (\n",
        "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3ZK-uzrW3Jv",
        "outputId": "bfc065c2-bea0-41bf-9f55-d3dd7345f0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.039174 M parameters\n"
          ]
        }
      ],
      "source": [
        "mt_transformer = EncDecTransformer(**model_hyparam).to(device)\n",
        "optimizer = torch.optim.AdamW(mt_transformer.parameters(), base_lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda step: learning_rate(step, \n",
        "                                                                        model_hyparam[\"model_dim\"], \n",
        "                                                                        factor=1, warmup=warmup_steps))\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in mt_transformer.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9R5ldo1W3Jv",
        "outputId": "b3faaf53-89ca-4c66-cdb1-83b0481c8bfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 0.07729663540734649, 'val': 0.06238006488096912}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def compute_loss():\n",
        "    mt_transformer.eval()\n",
        "    out_loss = {}\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        running_loss = 0.0\n",
        "        total_tokens = 0\n",
        "        for _ in range(eval_iters):\n",
        "            batch = build_batch(split, batch_size)\n",
        "            _, loss = mt_transformer(batch.src, \n",
        "                                batch.src_mask,\n",
        "                                batch.tgt,\n",
        "                                batch.tgt_mask,\n",
        "                                batch.tgt_y,\n",
        "                                batch.tgt_pad_mask)\n",
        "            running_loss += loss.item()\n",
        "            total_tokens += batch.n_tokens\n",
        "        out_loss[split] = running_loss/total_tokens\n",
        "    mt_transformer.train()\n",
        "    return out_loss\n",
        "compute_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXPEDLVvW3Jw"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "def train():\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    min_val_loss = float('inf')\n",
        "    for iter in range(n_iters):\n",
        "        mt_transformer.train()\n",
        "        batch = build_batch(\"train\", batch_size)\n",
        "        _, loss = mt_transformer(batch.src, \n",
        "                                    batch.src_mask,\n",
        "                                    batch.tgt,\n",
        "                                    batch.tgt_mask,\n",
        "                                    batch.tgt_y,\n",
        "                                    batch.tgt_pad_mask)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        if iter % eval_interval == 0 or iter == n_iters - 1:\n",
        "            losses = compute_loss()\n",
        "            train_losses.append(losses['train'])\n",
        "            val_losses.append(losses['val'])\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "            # save model checkpoint\n",
        "            if val_losses[-1] < min_val_loss:\n",
        "                min_val_loss = val_losses[-1]\n",
        "                torch.save(mt_transformer.state_dict(), \"mt_transformer.pt\")\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXPJCbHjY8Hp",
        "outputId": "c6de41ac-8f90-4724-a7d5-9b1d95b66561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 0.0742, val loss 0.0602\n",
            "step 10: train loss 0.0426, val loss 0.0398\n",
            "step 20: train loss 0.0354, val loss 0.0359\n",
            "step 30: train loss 0.0352, val loss 0.0364\n",
            "step 40: train loss 0.0342, val loss 0.0355\n",
            "step 50: train loss 0.0337, val loss 0.0353\n",
            "step 60: train loss 0.0328, val loss 0.0348\n",
            "step 70: train loss 0.0313, val loss 0.0336\n",
            "step 80: train loss 0.0304, val loss 0.0335\n",
            "step 90: train loss 0.0301, val loss 0.0334\n",
            "step 100: train loss 0.0298, val loss 0.0326\n",
            "step 110: train loss 0.0300, val loss 0.0325\n",
            "step 120: train loss 0.0293, val loss 0.0325\n",
            "step 130: train loss 0.0302, val loss 0.0320\n",
            "step 140: train loss 0.0291, val loss 0.0323\n",
            "step 150: train loss 0.0285, val loss 0.0315\n",
            "step 160: train loss 0.0285, val loss 0.0323\n",
            "step 170: train loss 0.0281, val loss 0.0315\n",
            "step 180: train loss 0.0279, val loss 0.0314\n",
            "step 190: train loss 0.0283, val loss 0.0321\n",
            "step 200: train loss 0.0279, val loss 0.0317\n",
            "step 210: train loss 0.0282, val loss 0.0312\n",
            "step 220: train loss 0.0278, val loss 0.0310\n",
            "step 230: train loss 0.0273, val loss 0.0311\n",
            "step 240: train loss 0.0274, val loss 0.0314\n",
            "step 250: train loss 0.0271, val loss 0.0321\n",
            "step 260: train loss 0.0271, val loss 0.0312\n",
            "step 270: train loss 0.0267, val loss 0.0309\n",
            "step 280: train loss 0.0272, val loss 0.0311\n",
            "step 290: train loss 0.0270, val loss 0.0307\n",
            "step 300: train loss 0.0272, val loss 0.0320\n",
            "step 310: train loss 0.0273, val loss 0.0323\n",
            "step 320: train loss 0.0270, val loss 0.0317\n",
            "step 330: train loss 0.0266, val loss 0.0310\n",
            "step 340: train loss 0.0263, val loss 0.0302\n",
            "step 350: train loss 0.0268, val loss 0.0309\n",
            "step 360: train loss 0.0259, val loss 0.0312\n",
            "step 370: train loss 0.0257, val loss 0.0312\n",
            "step 380: train loss 0.0262, val loss 0.0307\n",
            "step 390: train loss 0.0263, val loss 0.0302\n",
            "step 400: train loss 0.0259, val loss 0.0303\n",
            "step 410: train loss 0.0255, val loss 0.0304\n",
            "step 420: train loss 0.0256, val loss 0.0301\n",
            "step 430: train loss 0.0256, val loss 0.0298\n",
            "step 440: train loss 0.0253, val loss 0.0302\n",
            "step 450: train loss 0.0251, val loss 0.0307\n",
            "step 460: train loss 0.0249, val loss 0.0303\n",
            "step 470: train loss 0.0250, val loss 0.0310\n",
            "step 480: train loss 0.0251, val loss 0.0304\n",
            "step 490: train loss 0.0247, val loss 0.0300\n",
            "step 500: train loss 0.0251, val loss 0.0297\n",
            "step 510: train loss 0.0247, val loss 0.0306\n",
            "step 520: train loss 0.0247, val loss 0.0307\n",
            "step 530: train loss 0.0248, val loss 0.0304\n",
            "step 540: train loss 0.0249, val loss 0.0307\n",
            "step 550: train loss 0.0247, val loss 0.0305\n",
            "step 560: train loss 0.0246, val loss 0.0310\n",
            "step 570: train loss 0.0246, val loss 0.0300\n",
            "step 580: train loss 0.0242, val loss 0.0295\n",
            "step 590: train loss 0.0245, val loss 0.0302\n",
            "step 600: train loss 0.0239, val loss 0.0300\n",
            "step 610: train loss 0.0244, val loss 0.0297\n",
            "step 620: train loss 0.0245, val loss 0.0302\n",
            "step 630: train loss 0.0246, val loss 0.0304\n",
            "step 640: train loss 0.0238, val loss 0.0303\n",
            "step 650: train loss 0.0240, val loss 0.0301\n",
            "step 660: train loss 0.0238, val loss 0.0300\n",
            "step 670: train loss 0.0235, val loss 0.0296\n",
            "step 680: train loss 0.0237, val loss 0.0296\n",
            "step 690: train loss 0.0238, val loss 0.0302\n",
            "step 700: train loss 0.0241, val loss 0.0298\n",
            "step 710: train loss 0.0232, val loss 0.0292\n",
            "step 720: train loss 0.0246, val loss 0.0295\n",
            "step 730: train loss 0.0238, val loss 0.0304\n",
            "step 740: train loss 0.0234, val loss 0.0294\n",
            "step 750: train loss 0.0231, val loss 0.0292\n",
            "step 760: train loss 0.0236, val loss 0.0291\n",
            "step 770: train loss 0.0230, val loss 0.0294\n",
            "step 780: train loss 0.0233, val loss 0.0291\n",
            "step 790: train loss 0.0234, val loss 0.0296\n",
            "step 800: train loss 0.0229, val loss 0.0293\n",
            "step 810: train loss 0.0233, val loss 0.0290\n",
            "step 820: train loss 0.0229, val loss 0.0290\n",
            "step 830: train loss 0.0225, val loss 0.0294\n",
            "step 840: train loss 0.0229, val loss 0.0289\n",
            "step 850: train loss 0.0226, val loss 0.0297\n",
            "step 860: train loss 0.0229, val loss 0.0288\n",
            "step 870: train loss 0.0228, val loss 0.0295\n",
            "step 880: train loss 0.0222, val loss 0.0291\n",
            "step 890: train loss 0.0220, val loss 0.0295\n",
            "step 900: train loss 0.0224, val loss 0.0297\n",
            "step 910: train loss 0.0222, val loss 0.0291\n",
            "step 920: train loss 0.0221, val loss 0.0294\n",
            "step 930: train loss 0.0224, val loss 0.0293\n",
            "step 940: train loss 0.0227, val loss 0.0293\n",
            "step 950: train loss 0.0224, val loss 0.0288\n",
            "step 960: train loss 0.0221, val loss 0.0288\n",
            "step 970: train loss 0.0219, val loss 0.0285\n",
            "step 980: train loss 0.0219, val loss 0.0286\n",
            "step 990: train loss 0.0220, val loss 0.0284\n",
            "step 1000: train loss 0.0218, val loss 0.0289\n",
            "step 1010: train loss 0.0218, val loss 0.0294\n",
            "step 1020: train loss 0.0221, val loss 0.0291\n",
            "step 1030: train loss 0.0224, val loss 0.0291\n",
            "step 1040: train loss 0.0219, val loss 0.0293\n",
            "step 1050: train loss 0.0216, val loss 0.0294\n",
            "step 1060: train loss 0.0219, val loss 0.0288\n",
            "step 1070: train loss 0.0218, val loss 0.0286\n",
            "step 1080: train loss 0.0217, val loss 0.0288\n",
            "step 1090: train loss 0.0216, val loss 0.0291\n",
            "step 1100: train loss 0.0218, val loss 0.0284\n",
            "step 1110: train loss 0.0214, val loss 0.0287\n",
            "step 1120: train loss 0.0219, val loss 0.0289\n",
            "step 1130: train loss 0.0216, val loss 0.0291\n",
            "step 1140: train loss 0.0213, val loss 0.0284\n",
            "step 1150: train loss 0.0211, val loss 0.0287\n",
            "step 1160: train loss 0.0217, val loss 0.0284\n",
            "step 1170: train loss 0.0212, val loss 0.0286\n",
            "step 1180: train loss 0.0211, val loss 0.0288\n",
            "step 1190: train loss 0.0213, val loss 0.0286\n",
            "step 1200: train loss 0.0214, val loss 0.0287\n",
            "step 1210: train loss 0.0212, val loss 0.0287\n",
            "step 1220: train loss 0.0207, val loss 0.0284\n",
            "step 1230: train loss 0.0213, val loss 0.0285\n",
            "step 1240: train loss 0.0208, val loss 0.0284\n",
            "step 1250: train loss 0.0205, val loss 0.0282\n",
            "step 1260: train loss 0.0210, val loss 0.0287\n",
            "step 1270: train loss 0.0211, val loss 0.0286\n",
            "step 1280: train loss 0.0211, val loss 0.0286\n",
            "step 1290: train loss 0.0207, val loss 0.0285\n",
            "step 1300: train loss 0.0213, val loss 0.0291\n",
            "step 1310: train loss 0.0210, val loss 0.0283\n",
            "step 1320: train loss 0.0208, val loss 0.0282\n",
            "step 1330: train loss 0.0213, val loss 0.0293\n",
            "step 1340: train loss 0.0207, val loss 0.0283\n",
            "step 1350: train loss 0.0210, val loss 0.0282\n",
            "step 1360: train loss 0.0205, val loss 0.0284\n",
            "step 1370: train loss 0.0207, val loss 0.0279\n",
            "step 1380: train loss 0.0207, val loss 0.0284\n",
            "step 1390: train loss 0.0205, val loss 0.0287\n",
            "step 1400: train loss 0.0206, val loss 0.0286\n",
            "step 1410: train loss 0.0205, val loss 0.0284\n",
            "step 1420: train loss 0.0204, val loss 0.0281\n",
            "step 1430: train loss 0.0206, val loss 0.0286\n",
            "step 1440: train loss 0.0206, val loss 0.0284\n",
            "step 1450: train loss 0.0205, val loss 0.0283\n",
            "step 1460: train loss 0.0204, val loss 0.0280\n",
            "step 1470: train loss 0.0205, val loss 0.0283\n",
            "step 1480: train loss 0.0207, val loss 0.0282\n",
            "step 1490: train loss 0.0200, val loss 0.0277\n",
            "step 1500: train loss 0.0206, val loss 0.0278\n",
            "step 1510: train loss 0.0201, val loss 0.0275\n",
            "step 1520: train loss 0.0208, val loss 0.0279\n",
            "step 1530: train loss 0.0207, val loss 0.0283\n",
            "step 1540: train loss 0.0202, val loss 0.0282\n",
            "step 1550: train loss 0.0205, val loss 0.0279\n",
            "step 1560: train loss 0.0205, val loss 0.0281\n",
            "step 1570: train loss 0.0200, val loss 0.0282\n",
            "step 1580: train loss 0.0204, val loss 0.0276\n",
            "step 1590: train loss 0.0207, val loss 0.0283\n",
            "step 1600: train loss 0.0201, val loss 0.0286\n",
            "step 1610: train loss 0.0201, val loss 0.0284\n",
            "step 1620: train loss 0.0202, val loss 0.0280\n",
            "step 1630: train loss 0.0204, val loss 0.0283\n",
            "step 1640: train loss 0.0200, val loss 0.0276\n",
            "step 1650: train loss 0.0202, val loss 0.0279\n",
            "step 1660: train loss 0.0200, val loss 0.0284\n",
            "step 1670: train loss 0.0198, val loss 0.0281\n",
            "step 1680: train loss 0.0199, val loss 0.0284\n",
            "step 1690: train loss 0.0198, val loss 0.0283\n",
            "step 1700: train loss 0.0197, val loss 0.0282\n",
            "step 1710: train loss 0.0197, val loss 0.0285\n",
            "step 1720: train loss 0.0200, val loss 0.0277\n",
            "step 1730: train loss 0.0196, val loss 0.0280\n",
            "step 1740: train loss 0.0198, val loss 0.0276\n",
            "step 1750: train loss 0.0199, val loss 0.0279\n",
            "step 1760: train loss 0.0199, val loss 0.0280\n",
            "step 1770: train loss 0.0195, val loss 0.0278\n",
            "step 1780: train loss 0.0197, val loss 0.0284\n",
            "step 1790: train loss 0.0198, val loss 0.0285\n",
            "step 1800: train loss 0.0196, val loss 0.0277\n",
            "step 1810: train loss 0.0197, val loss 0.0285\n",
            "step 1820: train loss 0.0199, val loss 0.0282\n",
            "step 1830: train loss 0.0196, val loss 0.0284\n",
            "step 1840: train loss 0.0197, val loss 0.0281\n",
            "step 1850: train loss 0.0200, val loss 0.0287\n",
            "step 1860: train loss 0.0201, val loss 0.0274\n",
            "step 1870: train loss 0.0197, val loss 0.0278\n",
            "step 1880: train loss 0.0192, val loss 0.0274\n",
            "step 1890: train loss 0.0195, val loss 0.0280\n",
            "step 1900: train loss 0.0197, val loss 0.0278\n",
            "step 1910: train loss 0.0194, val loss 0.0281\n",
            "step 1920: train loss 0.0193, val loss 0.0286\n",
            "step 1930: train loss 0.0190, val loss 0.0277\n",
            "step 1940: train loss 0.0197, val loss 0.0281\n",
            "step 1950: train loss 0.0193, val loss 0.0277\n",
            "step 1960: train loss 0.0191, val loss 0.0279\n",
            "step 1970: train loss 0.0190, val loss 0.0284\n",
            "step 1980: train loss 0.0192, val loss 0.0275\n",
            "step 1990: train loss 0.0191, val loss 0.0274\n",
            "step 1999: train loss 0.0192, val loss 0.0278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sguMZWeW3Jw",
        "outputId": "dfb268ca-aab8-4513-b4e0-609740e8dc8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.019216531409506687, 0.027812004687292453)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "train_losses[-1], val_losses[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ENVenv5YW3Jw",
        "outputId": "2b9191fd-f4d2-49d6-cb4c-6f1f25a858d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff22aef5a30>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(train_losses, label=\"train\")\n",
        "plt.plot(val_losses, label=\"val\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "IM6ZnIOaW3Jw",
        "outputId": "c879e18f-9ea9-4b53-b827-d8378f9f0e1c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2086c5003091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{src_language}: {src_lang.tensor_to_sentence(batch.src[0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{target_language}: {target_lang.tensor_to_sentence(out[0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_batch' is not defined"
          ]
        }
      ],
      "source": [
        "batch = build_batch(\"test\", batch_size=1)\n",
        "print(f\"{src_language}: {src_lang.tensor_to_sentence(batch.src[0])}\")\n",
        "out = mt_transformer.generate(batch.src, batch.src_mask, max_tokens=11)\n",
        "print(f\"{target_language}: {target_lang.tensor_to_sentence(out[0])}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml_mps_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16 (main, Jan 11 2023, 10:02:19) \n[Clang 14.0.6 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "219301d06b24a901e45fde882757c1228fa788b1ab703da7f9c2352940fd5cfd"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}