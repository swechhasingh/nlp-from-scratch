{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-French Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, lang_name):\n",
    "        self.lang_name = lang_name\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word_to_count = {}\n",
    "        self.vocab_size = 2\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_index:\n",
    "            self.word_to_index[word] = self.vocab_size\n",
    "            self.index_to_word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "        self.word_to_count[word] = self.word_to_count.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'eng'\n",
    "target_lang = 'fra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.\\tVa !', 'Run!\\tCours\\u202f!', 'Run!\\tCourez\\u202f!', 'Wow!\\tÇa alors\\u202f!', 'Fire!\\tAu feu !']\n"
     ]
    }
   ],
   "source": [
    "# Read the file and split into lines\n",
    "with open('data/%s-%s.txt' % (src_lang, target_lang), encoding='utf-8') as file:\n",
    "    text_data = file.read().splitlines()\n",
    "print(text_data[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is in Unicode. So, we will take the following preprocessing steps:\n",
    "1. Turn Unicode characters to ASCII\n",
    "2. lowercase\n",
    "3. Trim most punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(sent):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', sent)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def preprocess_string(sent):\n",
    "    \"lowercase, unicode_to_ascii, trim, and remove non-letter characters\"\n",
    "    sent = sent.lower().strip()\n",
    "    sent = unicode_to_ascii(sent)\n",
    "    # The backreference \\1 (backslash one) references the first capturing group. \n",
    "    # space followed by \\1 matches the exact same text that was matched by the first capturing group [.!?].\n",
    "    sent = re.sub(r\"([.!?])\", r\" \\1\", sent)\n",
    "    # replace character which are not from this set (a-zA-Z.!?) by single space character\n",
    "    sent = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sent)\n",
    "    return sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow!\tÇa alors !\n",
      "['Ça alors\\u202f!', 'Wow!']\n"
     ]
    }
   ],
   "source": [
    "preprocess_string(text_data[3])\n",
    "print(text_data[3])\n",
    "print(text_data[3].split('\\t')[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, reverse=False):\n",
    "    print(\"Reading text file...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    with open('data/%s' % (file_name), encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "\n",
    "    # Split every line into pairs [src_lang, target_lang] and preprocess\n",
    "    pairs = [[preprocess_string(s) for s in line.split('\\t')] for line in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [p[::-1] for p in pairs]\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text file...\n"
     ]
    }
   ],
   "source": [
    "pairs = load_data('eng-fra.txt', reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['va !', 'go .'], ['cours !', 'run !']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "va ! go .\n",
      "cours ! run !\n",
      "courez ! run !\n",
      "ca alors ! wow !\n"
     ]
    }
   ],
   "source": [
    "# create language instances\n",
    "input_lang = Language(src_lang)\n",
    "output_lang = Language(target_lang)\n",
    "\n",
    "for src, target in pairs[:4]:\n",
    "    print(src, target)\n",
    "    input_lang.add_sentence(src)\n",
    "    output_lang.add_sentence(target) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "Read text file and split into lines, split lines into pairs\n",
    "\n",
    "Normalize text, filter by length and content\n",
    "\n",
    "Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text file...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 4345\n",
      "fra 2803\n",
      "['c est l un de mes voisins .', 'he is one of my neighbors .']\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(src_lang, target_lang, reverse=False):\n",
    "    pairs = load_data('eng-fra.txt', reverse)\n",
    "\n",
    "    # create language instances\n",
    "    input_lang = Language(src_lang)\n",
    "    output_lang = Language(target_lang)\n",
    "\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for src, target in pairs:\n",
    "        # print(src, target)\n",
    "        input_lang.add_sentence(src)\n",
    "        output_lang.add_sentence(target) \n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.lang_name, input_lang.vocab_size)\n",
    "    print(output_lang.lang_name, output_lang.vocab_size)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = create_dataset('eng', 'fra', reverse=True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02a75899b11e31bd85ccc905c76a128ad2bb11b6b68df04d39be7741d54ebc47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
