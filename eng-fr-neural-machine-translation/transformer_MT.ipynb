{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-French Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_language = 'eng'\n",
    "target_language = 'fra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.\\tVa !', 'Run!\\tCours\\u202f!', 'Run!\\tCourez\\u202f!', 'Wow!\\tÇa alors\\u202f!', 'Fire!\\tAu feu !']\n"
     ]
    }
   ],
   "source": [
    "# Read the file and split into lines\n",
    "with open('data/%s-%s.txt' % (src_language, target_language), encoding='utf-8') as file:\n",
    "    text_data = file.read().splitlines()\n",
    "print(text_data[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is in Unicode. So, we will take the following preprocessing steps:\n",
    "1. Turn Unicode characters to ASCII\n",
    "2. lowercase\n",
    "3. Trim most punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(sent):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', sent)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def preprocess_string(sent):\n",
    "    \"lowercase, unicode_to_ascii, trim, and remove non-letter characters\"\n",
    "    sent = sent.lower().strip()\n",
    "    sent = unicode_to_ascii(sent)\n",
    "    # The backreference \\1 (backslash one) references the first capturing group. \n",
    "    # space followed by \\1 matches the exact same text that was matched by the first capturing group [.!?].\n",
    "    sent = re.sub(r\"([.!?])\", r\" \\1\", sent)\n",
    "    # replace character which are not from this set (a-zA-Z.!?) by single space character\n",
    "    sent = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sent)\n",
    "    return sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow!\tÇa alors !\n",
      "['Ça alors\\u202f!', 'Wow!']\n"
     ]
    }
   ],
   "source": [
    "preprocess_string(text_data[3])\n",
    "print(text_data[3])\n",
    "print(text_data[3].split('\\t')[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, reverse=False):\n",
    "    print(\"Reading text file...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    with open('data/%s' % (file_name), encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "\n",
    "    # Split every line into pairs [src_lang, target_lang] and preprocess\n",
    "    pairs = [[preprocess_string(s) for s in line.split('\\t')] for line in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [p[::-1] for p in pairs]\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text file...\n"
     ]
    }
   ],
   "source": [
    "pairs = load_data('eng-fra.txt', reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK = 2\n",
    "PAD = 3\n",
    "BLOCK_SIZE = 12\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, lang_name, src=True):\n",
    "        self.lang_name = lang_name\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\", 3:\"PAD\"}\n",
    "        self.word_to_count = {}\n",
    "        self.vocab_size = 4\n",
    "        self.src = src\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_index:\n",
    "            self.word_to_index[word] = self.vocab_size\n",
    "            self.index_to_word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "        self.word_to_count[word] = self.word_to_count.get(word, 0) + 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def sentence_to_indexes(self, sentence):\n",
    "        idxs = [self.word_to_index[word] if word in self.word_to_index else self.word_to_index[\"UNK\"] for word in sentence.split(' ')]\n",
    "        return idxs\n",
    "        \n",
    "    def indexes_to_sentence(self, indexes):\n",
    "        return ' '.join([self.index_to_word[index] for index in indexes])\n",
    "\n",
    "    def sentence_to_tensor(self, sentence):\n",
    "        indexes = self.sentence_to_indexes(sentence)\n",
    "        indexes = [SOS_token] + indexes + [EOS_token]\n",
    "\n",
    "        max_len = BLOCK_SIZE if self.src else BLOCK_SIZE + 1\n",
    "        \n",
    "        if len(indexes) < max_len:\n",
    "            indexes += [PAD]*(max_len-len(indexes))\n",
    "        else:\n",
    "            indexes = indexes[:max_len]\n",
    "            \n",
    "        indexes = torch.tensor(indexes, dtype=torch.long)\n",
    "        return indexes\n",
    "\n",
    "    def tensor_to_sentence(self, idx_tensor):\n",
    "        if len(idx_tensor.shape) > 1:\n",
    "            idxs = idx_tensor.tolist()[0]\n",
    "        else:\n",
    "            idxs = idx_tensor.tolist()\n",
    "        sentence = self.indexes_to_sentence(idxs)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create language instances\n",
    "src_lang = Language(src_language)\n",
    "target_lang = Language(target_language)\n",
    "\n",
    "for src, target in pairs:\n",
    "    src_lang.add_sentence(src)\n",
    "    target_lang.add_sentence(target) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p, reverse=False):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1 if reverse else 0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs, reverse):\n",
    "    return [pair for pair in pairs if filterPair(pair, reverse)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "Read text file and split into lines, split lines into pairs\n",
    "\n",
    "Normalize text, filter by length and content\n",
    "\n",
    "Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(src_lang, target_lang, reverse=False):\n",
    "    pairs = load_data('eng-fra.txt', reverse)\n",
    "\n",
    "    # create language instances\n",
    "    input_lang = Language(src_lang)\n",
    "    output_lang = Language(target_lang, src=False)\n",
    "\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    pairs = filterPairs(pairs, reverse)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    # train/val/test split\n",
    "    n_total = len(pairs)\n",
    "    n_train = int(0.8*n_total)\n",
    "    n_val = int(0.1*n_total)\n",
    "    n_test = n_total - n_train - n_val\n",
    "    print(f\"{n_train=}, {n_val=}, {n_test=}\")\n",
    "    pair_split = {}\n",
    "    pair_split['train'] = pairs[:n_train]\n",
    "    pair_split['val'] = pairs[n_train:n_train + n_val]\n",
    "    pair_split['test'] = pairs[n_train + n_val:]\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    print(\"Creating source and target language vocab using pair_split['train']...\")\n",
    "    for src, target in pair_split['train']:\n",
    "        input_lang.add_sentence(src)\n",
    "        output_lang.add_sentence(target) \n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.lang_name, input_lang.vocab_size)\n",
    "    print(output_lang.lang_name, output_lang.vocab_size)\n",
    "    return input_lang, output_lang, pair_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text file...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "n_train=8479, n_val=1059, n_test=1061\n",
      "Counting words...\n",
      "Creating source and target language vocab using pair_split['train']...\n",
      "Counted words:\n",
      "eng 2184\n",
      "fra 3526\n",
      "['we re still not sure .', 'nous ne sommes toujours pas surs .']\n"
     ]
    }
   ],
   "source": [
    "src_lang, target_lang, pair_split = create_dataset('eng', 'fra', reverse=False)\n",
    "print(random.choice(pair_split[\"train\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization: Words to indexes\n",
    "In seq2seq task takes an input sequence (source seq) and outputs another sequence (target seq). In our case, we have an input sentence in English language and a corresponding translated sentence in French language. These sentence need to converted into numbers (integers) to be able to into to a neural network. For this, we will use **word-level tokenization**, i.e *word to integer* index mapping.\n",
    "\n",
    "We need some special tokens to indicate start (SOS) and end (EOS) of a sentence. For the input sequence (source seq), the model needs to know when the input has ended and for the target sequence, the model needs to know when to start and when to end.\n",
    "\n",
    "So, we will append the EOS token to the end of input sentence and wrap the target sentence by SOS (in the beginning) and the EOS (in the end) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sent_pair_to_tensor_pair(pair, src_lang, target_lang):\n",
    "    input_tensor = src_lang.sentence_to_tensor(pair[0])\n",
    "    target_tensor = target_lang.sentence_to_tensor(pair[1])\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "def tensor_pair_to_sent_pair(pair, src_lang, target_lang):\n",
    "    input_tensor = src_lang.tensor_to_sentence(pair[0])\n",
    "    target_tensor = target_lang.tensor_to_sentence(pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you re not bruised .', 'vous n etes pas contusionnes .']\n",
      "(tensor([   0,  131,   80,  149, 1123,    6,    1,    3,    3,    3,    3,    3]), tensor([   0,  120,  247,  216,  248, 2008,    7,    1,    3,    3,    3,    3,\n",
      "           3]))\n",
      "(tensor([   0,  131,   80,  149, 1123,    6,    1,    3,    3,    3,    3,    3]), tensor([   0,  120,  247,  216,  248, 2008,    7,    1,    3,    3,    3,    3,\n",
      "           3]))\n",
      "('SOS you re not bruised . EOS PAD PAD PAD PAD PAD', 'SOS vous n etes pas contusionnes . EOS PAD PAD PAD PAD PAD')\n"
     ]
    }
   ],
   "source": [
    "pair = random.choices(pair_split[\"train\"], k=1)[0]\n",
    "print(pair)\n",
    "print(sent_pair_to_tensor_pair(pair, src_lang, target_lang))\n",
    "print(sent_pair_to_tensor_pair(pair, src_lang, target_lang))\n",
    "\n",
    "p1, p2 = sent_pair_to_tensor_pair(pair, src_lang, target_lang)\n",
    "print(tensor_pair_to_sent_pair((p1,p2), src_lang, target_lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, target=None, pad_idx=2) -> None:\n",
    "        # src and target sequences have same length\n",
    "        self.src = src #shape:(B,T)\n",
    "        # src_mask:(B,1,1,T)\n",
    "        self.src_mask = (src != pad_idx).unsqueeze(-2).unsqueeze(-2)\n",
    "        if target is not None:\n",
    "            # decoder output shifted by one\n",
    "            self.tgt = target[:,:-1] #shape:(B,T)\n",
    "            self.tgt_y = target[:,1:]\n",
    "            self.tgt_mask = self.causal_mask(self.tgt, pad_idx)\n",
    "            self.n_tokens = (self.tgt_y != pad_idx).sum().item()\n",
    "\n",
    "    @staticmethod   \n",
    "    def causal_mask(target, pad_idx):\n",
    "        # max context length = block_size\n",
    "        T = target.shape[1]\n",
    "        # causal attention mask: (1,T,T)\n",
    "        causal_attn_mask = torch.tril(torch.ones(1, T, T, dtype=torch.bool, device=target.device))\n",
    "        # padding mask: (B,1,T)\n",
    "        target_mask = (target != pad_idx).unsqueeze(-2)\n",
    "        target_mask = target_mask & causal_attn_mask\n",
    "        return target_mask.unsqueeze(1) # (B,1,T,T)\n",
    "\n",
    "        \n",
    "def build_batch(split, batch_size=4):\n",
    "    if split == \"train\":\n",
    "        pairs = pair_split[\"train\"]\n",
    "    elif split == \"val\":\n",
    "        pairs = pair_split[\"val\"]\n",
    "    else:\n",
    "        pairs = pair_split[\"test\"]\n",
    "    # randomly (uniformly) sample a start index for a sentence of length block_size\n",
    "    # number of sequences in a batch is batch_size\n",
    "    batch_pairs = random.choices(pairs, k=batch_size)\n",
    "    # input \n",
    "    src_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for pair in batch_pairs:\n",
    "        src_tensor, target_tensor = sent_pair_to_tensor_pair(pair, src_lang, target_lang)\n",
    "        src_batch.append(src_tensor)\n",
    "        target_batch.append(target_tensor)\n",
    "    \n",
    "    src = torch.stack(src_batch).to(device)\n",
    "    target = torch.stack(target_batch).to(device)\n",
    "\n",
    "    batch = Batch(src, target, PAD)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = build_batch(\"train\", batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS i m a good taxi driver . EOS PAD PAD PAD\n",
      "SOS je suis un bon chauffeur de taxi . EOS PAD PAD\n",
      "je suis un bon chauffeur de taxi . EOS PAD PAD PAD\n"
     ]
    }
   ],
   "source": [
    "print(src_lang.tensor_to_sentence(b.src))\n",
    "print(target_lang.tensor_to_sentence(b.tgt))\n",
    "print(target_lang.tensor_to_sentence(b.tgt_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq English-French Machine Translation Transformer Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from transformers.transformer import EncDecTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyparam = {\n",
    "    \"src_vocab_size\": src_lang.vocab_size,\n",
    "    \"tgt_vocab_size\": target_lang.vocab_size,\n",
    "    \"block_size\": BLOCK_SIZE,\n",
    "    \"model_dim\": 32,\n",
    "    \"n_layer\": 2,\n",
    "    \"n_head\": 2,\n",
    "    \"cross_attention\":False\n",
    "}\n",
    "mt_transformer = EncDecTransformer(**hyparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Batch at 0x7f8d2992d9d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = build_batch(\"train\", batch_size=4)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 12, 12]), torch.Size([4, 1, 1, 12]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.tgt_mask.shape, batch.src_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5063, -0.0208,  1.4968,  ...,  0.4326,  0.3583,  0.6695],\n",
       "         [-1.1774,  0.0886, -0.4037,  ..., -0.3972,  0.2675,  0.4055],\n",
       "         [-0.1008,  0.1045,  0.9398,  ..., -0.4221,  0.5021, -0.1748],\n",
       "         ...,\n",
       "         [ 0.4470,  0.1745,  0.5228,  ..., -0.4917, -0.8012, -0.3801],\n",
       "         [ 1.1149,  0.2723, -0.0676,  ...,  1.3096,  0.4201,  0.4185],\n",
       "         [ 0.8803,  0.3873,  0.2028,  ..., -0.0410,  0.4175, -0.3633]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(8.4518, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_transformer(batch.src, \n",
    "            batch.src_mask,\n",
    "            batch.tgt,\n",
    "            batch.tgt_mask,batch.tgt_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0,   4,   5,  71, 147, 749,  42, 239,   6,   1,   3,   3]]),\n",
       " tensor([[[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            False, False]]]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.src[0:1], batch.src_mask[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1, 12])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.src_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 2111, 1307, 3195, 2951, 2166,  974,  195, 2922, 2956, 3209, 2233]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mt_transformer.generate(batch.src[0:1], batch.src_mask[0:1], max_tokens=11)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS soir charmante evanouir cachez ravissante habitue gentille rendent rates hurle effrontees\n"
     ]
    }
   ],
   "source": [
    "print(target_lang.tensor_to_sentence(out[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02a75899b11e31bd85ccc905c76a128ad2bb11b6b68df04d39be7741d54ebc47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
